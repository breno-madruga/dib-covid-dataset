{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning and Preprocessing the Scopus publications related to COVID-19"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For collecting the Scopus publications related to COVID-19, we used the \"pybliometrics\" library. It is avaliable on https://pypi.org/project/pybliometrics/."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Importing the required libraries.\n",
    "import re, csv, pandas as pd, numpy as np\n",
    "from pylatexenc.latex2text import LatexNodes2Text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Generating the dataframe from the raw data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a dataframe from the raw data.\n",
    "df_data = pd.read_csv(\"../../data/raw/scopus_raw.csv\", header=0, dtype=object)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking the dataframe.\n",
    "df_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizing the information of dataset.\n",
    "df_data.info()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Cleaning and preprocessing the dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining the function \"clean_text\" to clean and preprocess any text.\n",
    "def clean_text(text, has_latex=False):\n",
    "    if text:\n",
    "        text = re.sub(r\"\\u2fff(s|\\s)\", r\"'\\1\", re.sub(r\"\\s+\", \" \", re.sub(r\"\\ufeff\\.?\", \"\", re.sub(\n",
    "            r\"\\\\\\\\(\\’\\s)?\", \"\", str(text))))).replace(\"\\u200b\", \"\").replace(\"\\ue001\", \"\").replace(\n",
    "            \"\\ue061\", \"\").replace(\"\\u202f\", \"\").replace(\"\\u2060\", \"\").replace(\"\\u200f\", \"\").replace(\n",
    "            \"\\u200e\", \"\").replace(\"\\u202c\", \"\").replace(\"&#x2013;\", \"-\").replace(\"&quot\", \"\\\"\\\"\").replace(\n",
    "            \"\\u200c\", \"\").replace(\"\\\\u0019\", \"\").replace(\"\\\\s\", \"s\").replace(\"\\u202a\", \"\").replace(\n",
    "            \"\\u202d\", \"-\").replace(\"\\u0383\", \"-\").replace(\"\\u20f3\", \"ó\").replace(\"\\u20fa\", \"ú\").replace(\n",
    "            \"\\u2fff\", \"-\").strip()\n",
    "        text = text.replace(\"TNF-alpha induced\", \"TNF-α induced\").replace(\n",
    "            \"TNF-Alpha induced\", \"TNF-α induced\").replace(\"TNF- ␣ induced\", \"TNF-α induced\").replace(\n",
    "            \"TNF-αinduced\", \"TNF-α induced\").replace(\n",
    "            \"via NF- \\u242c B pathway\", \"via NF-κB pathway\").replace(\n",
    "            \"via NF-kappaB pathway\", \"via NF-κB pathway\").strip()\n",
    "        if has_latex:\n",
    "            text = LatexNodes2Text().latex_to_text(re.sub(\"\\\\?%\", \"@PER@CENT@\", text)).replace(\"@PER@CENT@\", \"%\")\n",
    "        text = re.sub(r\"\\s+\", \" \", re.sub(r\"\\-{2,}\", \"-\", re.sub(r\"\\s?\\xad(\\s|\\-)?\", \"-\", text))).replace(\n",
    "            \"\\\\\", \"\").replace(\"\\\\%\", \"%\").replace(\"()\", \"\").replace(\"[]\", \"\").strip()\n",
    "        return text\n",
    "    else:\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing the invalid articles.\n",
    "df_data = df_data.loc[df_data.id.notnull() & df_data.eid.notnull()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining the \"None\" value for the \"NaN\" values.\n",
    "df_data.replace({np.nan: None}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining the \"zero\" value for the articles without numbers of citation and references.\n",
    "df_data.citation_num.loc[df_data.citation_num.isnull()] = 0\n",
    "df_data.ref_count.loc[df_data.ref_count.isnull()] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing the feature \"abstract\".\n",
    "df_data.abstract.loc[df_data.abstract.isnull() & df_data.description.notnull()] = df_data.description.loc[\n",
    "    df_data.abstract.isnull() & df_data.description.notnull()]\n",
    "df_data.abstract.loc[df_data.abstract.notnull()] = df_data.abstract.loc[df_data.abstract.notnull()].apply(\n",
    "    lambda x: clean_text(x, True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing the feature \"vehicle_name\".\n",
    "df_data.vehicle_name.loc[df_data.conference_name.notnull() & df_data.vehicle_name.notnull()] = df_data.conference_name.loc[df_data.conference_name.notnull() & df_data.vehicle_name.notnull()]\n",
    "df_data.vehicle_name.loc[df_data.vehicle_name.notnull()] = df_data.vehicle_name.loc[\n",
    "    df_data.vehicle_name.notnull()].apply(clean_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing the feature \"title\".\n",
    "df_data.title.loc[df_data.title.notnull()] = df_data.title.loc[df_data.title.notnull()].apply(clean_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing unnecessary columns.\n",
    "columns_drop = [\"eid\", \"pii\", \"description\", \"isbn\", \"conf_location\", \"conference_name\",\n",
    "    \"vehicle_address\", \"title_edition\"]\n",
    "df_data.drop(axis=1, columns=columns_drop, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Changing the type of some features.\n",
    "df_data.loc[:, [\"citation_num\", \"ref_count\"]] = df_data.loc[\n",
    "    :, [\"citation_num\", \"ref_count\"]].astype(np.float32)\n",
    "df_data.auth_keywords.loc[df_data.auth_keywords.notnull()] = df_data.auth_keywords.loc[\n",
    "    df_data.auth_keywords.notnull()].apply(eval)\n",
    "df_data.index_terms.loc[df_data.index_terms.notnull()] = df_data.index_terms.loc[\n",
    "    df_data.index_terms.notnull()].apply(eval)\n",
    "df_data.affiliations.loc[df_data.affiliations.notnull()] = df_data.affiliations.loc[\n",
    "    df_data.affiliations.notnull()].apply(eval)\n",
    "df_data.subject_areas.loc[df_data.subject_areas.notnull()] = df_data.subject_areas.loc[\n",
    "    df_data.subject_areas.notnull()].apply(eval)\n",
    "df_data.authors.loc[df_data.authors.notnull()] = df_data.authors.loc[df_data.authors.notnull()].apply(eval)\n",
    "df_data.author_affil.loc[df_data.author_affil.notnull()] = df_data.author_affil.loc[\n",
    "    df_data.author_affil.notnull()].apply(eval)\n",
    "df_data.references.loc[df_data.references.notnull()] = df_data.references.loc[\n",
    "    df_data.references.notnull()].apply(eval)\n",
    "df_data.publication_date = pd.to_datetime(df_data.publication_date)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating the feature \"period\" from the feature \"publication_date\".\n",
    "if \"period\" not in df_data:\n",
    "    df_data[\"period\"] = df_data.publication_date.apply(lambda x: \"{}-{}\".format(x.year, x.month))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing the itens contained in the features \"auth_keywords\" and \"index_terms\".\n",
    "df_data.auth_keywords.loc[df_data.auth_keywords.notnull()] = df_data.auth_keywords.loc[\n",
    "    df_data.auth_keywords.notnull()].apply(lambda x: tuple([clean_text(item) for item in x]))\n",
    "df_data.index_terms.loc[df_data.index_terms.notnull()] = df_data.index_terms.loc[\n",
    "    df_data.index_terms.notnull()].apply(lambda x: tuple([clean_text(item) for item in x]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking there are invalid values in the features \"auth_keywords\", \"index_terms\" and \"subject_areas\".\n",
    "for column in [\"auth_keywords\", \"index_terms\", \"subject_areas\"]:\n",
    "    count = df_data.loc[df_data[column].notnull(), column][\n",
    "                [np.any([item == None or item.lower() == \"none\" for item in items])\n",
    "                 for items in df_data.loc[df_data[column].notnull(), column]]].size\n",
    "    print(\"{}: {}\".format(column, count))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing the invalid values in the features \"auth_keywords\", \"index_terms\" and \"subject_areas\".\n",
    "for column in [\"auth_keywords\", \"index_terms\", \"subject_areas\"]:\n",
    "    df_data.loc[df_data[column].notnull(), column] = [\n",
    "        tuple([item for item in items if item])\n",
    "        for items in df_data.loc[df_data[column].notnull(), column]]\n",
    "    df_data.loc[df_data[column].notnull(), column] = df_data.loc[\n",
    "        df_data[column].notnull(), column].apply(lambda x: x if len(x) > 0 else None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing the content contained in the features \"authors\", \"affiliations\" and \"author_affil\".\n",
    "df_data.affiliations.loc[df_data.affiliations.notnull()] = df_data.affiliations.loc[\n",
    "    df_data.affiliations.notnull()].apply(lambda x: tuple([{\"id\": item[\"id\"],\n",
    "        \"affiliation\": clean_text(item[\"affiliation\"]), \"country\": item[\"country\"]}\n",
    "        for item in x if item[\"id\"]]))\n",
    "df_data.author_affil.loc[df_data.author_affil.notnull()] = df_data.author_affil.loc[\n",
    "    df_data.author_affil.notnull()].apply(lambda x: tuple(\n",
    "        [{\"id\": item[\"id\"], \"name\": clean_text(item[\"name\"]), \"affil_id\": item[\"affil_id\"],\n",
    "          \"affiliation\": clean_text(item[\"affiliation\"]), \"country\": item[\"country\"]}\n",
    "         for item in x if item[\"id\"] or item[\"name\"] or item[\"affil_id\"] or \\\n",
    "             item[\"affiliation\"] or item[\"country\"]]))\n",
    "df_data.authors.loc[df_data.authors.notnull()] = df_data.authors.loc[\n",
    "    df_data.authors.notnull()].apply(lambda x: tuple(\n",
    "        [{\"id\": item[\"id\"], \"name\": clean_text(item[\"name\"])} for item in x if item[\"id\"]]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing the invalid values in the features \"authors\", \"affiliations\" and \"author_affil\".\n",
    "for column in [\"authors\", \"affiliations\", \"author_affil\"]:\n",
    "    df_data.loc[df_data[column].notnull(), column] = df_data.loc[\n",
    "        df_data[column].notnull(), column].apply(lambda x: x if len(x) > 0 else None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating the affiliations' and authors' IDs for those that have not a ID.\n",
    "df_data.author_affil.loc[df_data.author_affil.notnull()] = df_data.author_affil.loc[\n",
    "    df_data.author_affil.notnull()].apply(lambda x: tuple([{\n",
    "        \"id\": item[\"id\"] if item[\"id\"] and item[\"name\"] else \\\n",
    "            str(hash(\"{} - {}\".format(item[\"name\"], \"Scopus\"))) if item[\"name\"] else None,\n",
    "        \"name\": item[\"name\"],\n",
    "        \"affil_id\": item[\"affil_id\"] if item[\"affil_id\"] and item[\"affiliation\"] else \\\n",
    "            str(hash(\"{} - {}\".format(item[\"affiliation\"], \"Scopus\"))) \\\n",
    "                if item[\"affiliation\"] else None,\n",
    "        \"affiliation\": item[\"affiliation\"], \"country\": item[\"country\"]}\n",
    "    for item in x]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing duplicates within the list of affiliations and authors.\n",
    "df_data.author_affil.loc[df_data.author_affil.notnull()] = [\n",
    "    set([(au[\"id\"], au[\"name\"], au[\"affil_id\"],\n",
    "        au[\"affiliation\"], au[\"country\"]) for au in row])\n",
    "    for row in df_data.author_affil[df_data.author_affil.notnull()]]\n",
    "df_data.author_affil.loc[df_data.author_affil.notnull()] = [tuple([dict(zip(\n",
    "        [\"id\", \"name\", \"affil_id\", \"affiliation\", \"country\"], au)) for au in row])\n",
    "    for row in df_data.author_affil[df_data.author_affil.notnull()]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing the duplicated records by feature \"id\".\n",
    "df_data = df_data.sort_values(by=[\"id\", \"period\"]).drop_duplicates(\"id\", keep=\"first\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing the duplicated records by features \"title\" and \"doi\".\n",
    "df_data = pd.concat([df_data[df_data.title.isnull() | df_data.doi.isnull()],\n",
    "    df_data[df_data.title.notnull() & df_data.doi.notnull()].sort_values(\n",
    "        by=[\"title\", \"citation_num\", \"publication_date\"]).drop_duplicates(\n",
    "            [\"title\", \"doi\"], \"last\")], ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing the feature \"references\".\n",
    "df_data.references.loc[df_data.references.notnull()] = df_data.references.loc[\n",
    "    df_data.references.notnull()].apply(lambda x: tuple(\n",
    "        [{\"id\": ref[\"id\"], \"title\": clean_text(ref[\"title\"], True),\n",
    "          \"doi\": clean_text(ref[\"doi\"]), \"authors\": clean_text(ref[\"authors\"], True)}\n",
    "         for ref in x]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking the result.\n",
    "df_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizing the information of dataset.\n",
    "df_data.info()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Saving the dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Exporting the data to CSV file.\n",
    "df_data.to_csv(\"../../data/prepared/scopus_covid_19.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "f50bd5474255f82aa829301912ce59e29110123be660cf8d7583f66a20371684"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}