{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning and Preprocessing the PubMed publications related to COVID-19"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For collecting the PubMed publications related to COVID-19, we used the \"pymed\" library. It is avaliable on https://pypi.org/project/pymed/."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Uncomment to install the library.\n",
    "# %pip install pylatexenc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Importing the required libraries.\n",
    "import re, csv, pandas as pd, numpy as np\n",
    "from pylatexenc.latex2text import LatexNodes2Text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Generating the dataframe from the raw data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a dataframe from the raw data.\n",
    "df_data = pd.read_csv(\"../../data/raw/pubmed_raw.csv\", header=0, dtype=object)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking the dataframe.\n",
    "df_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizing the information of dataset.\n",
    "df_data.info()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Cleaning and preprocessing the dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining the function \"clean_text\" to clean and preprocess any text.\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        return re.sub(r\"\\\\\", \" \", re.sub(r\"\\s+\", \" \", re.sub(r\"\\-{2,}\", \"-\", re.sub(\"[0-9]*\\u200b\", \"\",\n",
    "            str(text)).replace(\"\\xad\", \"-\")).replace(\"\\u2009\", \" \").replace(\"\\xa0\", \" \").replace(\n",
    "            \"\\n\", \" \").replace(\"\\ufeff\", \"\").replace(\"\\u202f\", \"\").replace(\"\\u2028\", \" \").replace(\n",
    "            \"\\u200f\", \"\").replace(\"\\u200e\", \"\").replace(\"()\", \"\").replace(\"[]\", \"\").replace(\n",
    "            \"\\\\'\", \"\\'\").replace(\"\\uf06b\", \"\").replace(\"\\x96\", \"\").replace(\"\\u200c\", \"\"))).strip()\n",
    "    else:\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining the \"None\" value for the \"NaN\" values.\n",
    "df_data.replace({np.nan: None}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing unnecessary columns.\n",
    "columns_drop = [\"methods\", \"conclusions\", \"results\", \"copyrights\", \"xml\", \"isbn\",\n",
    "                \"language\", \"publication_type\", \"sections\", \"publisher\", \"publisher_location\"]\n",
    "df_data.drop(axis=1, columns=columns_drop, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Getting the PubMed ID for each paper.\n",
    "df_data.pubmed_id = df_data.pubmed_id.apply(lambda x: x.split()[0].strip())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing the doi for each paper.\n",
    "df_data.loc[df_data.doi.notnull(), \"doi\"] = df_data.loc[df_data.doi.notnull(), \"doi\"].apply(\n",
    "    lambda x: x.split()[0].strip())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing the features \"abstract\", \"title\" and \"journal\".\n",
    "df_data.abstract = df_data.abstract.apply(\n",
    "    lambda x: clean_text(LatexNodes2Text().latex_to_text(\n",
    "        re.sub(r\"\\s+\", \" \", re.sub(\"%\", \"\\\\%\", x)))) if x and len(x) > 0 else None)\n",
    "df_data.title = df_data.title.apply(lambda x: clean_text(x) if x and len(x) > 0 else None)\n",
    "df_data.journal = df_data.journal.apply(clean_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting the feature \"keywords\" as a tuple of keywords and normalizing the keywords for each paper.\n",
    "df_data.keywords.loc[df_data.keywords.notnull()] = [\n",
    "    tuple([clean_text(keyword) for keyword in eval(keywords)]) if eval(keywords) else None\n",
    "    for keywords in df_data.keywords[df_data.keywords.notnull()]]"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking there are invalid keywords.\n",
    "df_data[df_data.keywords.notnull()].keywords[\n",
    "    [np.any([item == None for item in keywords])\n",
    "    for keywords in df_data[df_data.keywords.notnull()].keywords]].size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing the invalid keywords.\n",
    "df_data.keywords.loc[df_data.keywords.notnull()] = [tuple([item for item in keywords if item])\n",
    "    for keywords in df_data.keywords[df_data.keywords.notnull()]]\n",
    "df_data.keywords.loc[df_data.keywords.notnull()] = df_data.keywords.loc[\n",
    "    df_data.keywords.notnull()].apply(lambda x: x if len(x) > 0 else None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking again there are invalid keywords.\n",
    "df_data[df_data.keywords.notnull()].keywords[\n",
    "    [np.any([item == None for item in keywords])\n",
    "    for keywords in df_data[df_data.keywords.notnull()].keywords]].size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Correcting the feature \"authors\".\n",
    "for idx, authors in enumerate(df_data.authors):\n",
    "    if not eval(authors):\n",
    "        df_data.authors[idx] = None\n",
    "    else:\n",
    "        list_authors = []\n",
    "        for author in eval(authors):\n",
    "            auth = {}\n",
    "            if author[\"firstname\"] and author[\"lastname\"]:\n",
    "                auth[\"name\"] = clean_text(\"{} {}\".format(author[\"firstname\"], author[\"lastname\"]))\n",
    "            elif author[\"firstname\"] and not author[\"lastname\"]:\n",
    "                auth[\"name\"] = clean_text(author[\"firstname\"])\n",
    "            elif not author[\"firstname\"] and author[\"lastname\"]:\n",
    "                auth[\"name\"] = clean_text(author[\"lastname\"])\n",
    "            else:\n",
    "                auth[\"name\"] = None\n",
    "\n",
    "            auth[\"id\"] = str(hash(\"{} - {}\".format(auth[\"name\"], \"PubMed\"))) if auth[\"name\"] else None\n",
    "            auth[\"affiliation\"] = clean_text(author[\"affiliation\"]) if \"affiliation\" in author else None\n",
    "            auth[\"affil_id\"] = str(hash(\"{} - {}\".format(auth[\"affiliation\"], \"PubMed\"))) \\\n",
    "                if auth[\"affiliation\"] else None\n",
    "            auth[\"country\"] = None\n",
    "\n",
    "            if auth[\"affiliation\"] or auth[\"name\"]:\n",
    "                list_authors.append(auth)\n",
    "\n",
    "        df_data.authors[idx] = tuple(list_authors) if len(list_authors) > 0 else None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Renaming the features \"authors\", \"keywords\" and \"journal\".\n",
    "df_data.rename(columns={\"authors\": \"author_affil\", \"keywords\": \"auth_keywords\",\n",
    "                        \"journal\": \"vehicle_name\"}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing the duplicated records by features \"title\" and \"doi\".\n",
    "df_data = pd.concat([df_data[df_data.title.isnull() | df_data.doi.isnull()],\n",
    "    df_data[df_data.title.notnull() & df_data.doi.notnull()].sort_values(\n",
    "        by=[\"title\", \"publication_date\"]).drop_duplicates([\"title\", \"doi\"], \"last\")], ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking the result.\n",
    "df_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizing the information of dataset.\n",
    "df_data.info()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Saving the dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Exporting the data to CSV file.\n",
    "df_data.to_csv(\"../../data/prepared/pubmed_covid_19.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "f50bd5474255f82aa829301912ce59e29110123be660cf8d7583f66a20371684"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}