{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Collecting the arXiv publications related to COVID-19"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The publications' data were collected from [arXiv webpage](https://arxiv.org/covid19search) related to COVID-19."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Importing the required libraries.\n",
    "import scrapy, re, csv, pandas as pd\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy import Selector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Getting the data from its URL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Determining the URL of target page.\n",
    "url = \"https://arxiv.org/covid19search\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating the repository of data.\n",
    "data = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Definition of Spider class.\n",
    "class SpiderArXiv(scrapy.Spider):\n",
    "    name = \"arXiv_covid\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        # Getting the list of papers contained in the first page.\n",
    "        args = dict(css = \"ol.breathe-horizontal > li.arxiv-result\")\n",
    "        yield scrapy.Request(url = url, callback=self.parse_paper, cb_kwargs=args)\n",
    "\n",
    "    def parse_paper(self, response, css):\n",
    "        # Extracting the list of papers.\n",
    "        papers = response.css(css).extract()\n",
    "\n",
    "        # Creating the list of CSS Selector.\n",
    "        css_list = {\"id\": \"p.list-title > a::text\",\n",
    "                    \"subject_areas\": \"div.tags > span.tag::attr(data-tooltip)\",\n",
    "                    \"title\": \"p.title ::text\",\n",
    "                    \"authors\": \"p.authors > a::text\",\n",
    "                    \"abstract\": \"p.abstract > span.abstract-full ::text\",\n",
    "                    \"date\": \"p.is-size-7::text\"}\n",
    "\n",
    "        # Extracting the data from paper's HTML.\n",
    "        for paper in papers:\n",
    "            sel = Selector(text=paper)\n",
    "            record = {}\n",
    "            record[\"id\"] = sel.css(css_list[\"id\"]).extract_first()\n",
    "            record[\"subject_areas\"] = sel.css(css_list[\"subject_areas\"]).extract()\n",
    "            record[\"title\"] = \"\".join(sel.css(css_list[\"title\"]).extract()).strip().replace(\"\\n\", \"\")\n",
    "            record[\"authors\"] = sel.css(css_list[\"authors\"]).extract()\n",
    "            record[\"abstract\"] = re.sub(r\"\\s+\", \" \", \"\".join(\n",
    "                sel.css(css_list[\"abstract\"]).extract()).strip().replace(\"â–³ Less\", \"\"))\n",
    "            record[\"date\"] = \"\".join(sel.css(css_list[\"date\"]).extract()).strip().replace(\"\\n\", \"\")\n",
    "            data.append(record)\n",
    "\n",
    "        # Extracting the URL within the button \"Next\".\n",
    "        link = response.css(\"a.pagination-next::attr(href)\").extract_first()\n",
    "\n",
    "        # Getting the list of papers contained in the next page.\n",
    "        if link:\n",
    "            args = dict(css = \"ol.breathe-horizontal > li.arxiv-result\")\n",
    "            yield response.follow(url = link, callback=self.parse_paper, cb_kwargs=args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Executing the spider.\n",
    "process = CrawlerProcess()\n",
    "process.crawl(SpiderArXiv)\n",
    "process.start()"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Printing the number of records collected.\n",
    "print(\"Number of records collected: {}.\".format(len(data)))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Saving the data collected"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Exporting the data to CSV file.\n",
    "pd.DataFrame(data).to_csv(\"../../data/raw/arxiv_raw.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0c1d2fcf5e2cc78a0662b1ea15472a8f3e1f5d200e2ea389ebabb285ef6ada5a6"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}